{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08767d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 InterDigital Communications, Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402126fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from compressai.datasets import ImageFolder\n",
    "from compressai.zoo import models\n",
    "\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b6cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateDistortionLossMSE(nn.Module):\n",
    "    \"\"\"Custom rate distortion loss with a Lagrangian parameter.\"\"\"\n",
    "\n",
    "    def __init__(self, lmbda=1e-2):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        N, _, H, W = target.size()\n",
    "        out = {}\n",
    "        num_pixels = N * H * W\n",
    "\n",
    "        out[\"bpp_loss\"] = sum(\n",
    "            (torch.log(likelihoods).sum() / (-math.log(2) * num_pixels))\n",
    "            for likelihoods in output[\"likelihoods\"].values()\n",
    "        )\n",
    "        out[\"mse_loss\"] = self.mse(output[\"x_hat\"], target)\n",
    "        out[\"loss\"] = self.lmbda * 255 ** 2 * out[\"mse_loss\"] + out[\"bpp_loss\"]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0fd9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateDistortionLossMSSSIM(nn.Module):\n",
    "    \"\"\"Custom rate distortion loss with a Lagrangian parameter.\"\"\"\n",
    "\n",
    "    def __init__(self, lmbda=1e-2):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.ms_ssim = ms_ssim\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        N, _, H, W = target.size()\n",
    "        out = {}\n",
    "        num_pixels = N * H * W\n",
    "\n",
    "        out[\"bpp_loss\"] = sum(\n",
    "            (torch.log(likelihoods).sum() / (-math.log(2) * num_pixels))\n",
    "            for likelihoods in output[\"likelihoods\"].values()\n",
    "        )\n",
    "        out[\"msssim_loss\"] = 1 - self.ms_ssim(output[\"x_hat\"], target, data_range=1.0, size_average=True)\n",
    "        out[\"loss\"] = self.lmbda * out[\"msssim_loss\"] + out[\"bpp_loss\"]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f430f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Compute running average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6952cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataParallel(nn.DataParallel):\n",
    "    \"\"\"Custom DataParallel to access the module methods.\"\"\"\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        try:\n",
    "            return super().__getattr__(key)\n",
    "        except AttributeError:\n",
    "            return getattr(self.module, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e95d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(net, learning_rate, aux_learning_rate):\n",
    "    \"\"\"Separate parameters for the main optimizer and the auxiliary optimizer.\n",
    "    Return two optimizers\"\"\"\n",
    "\n",
    "    parameters = {\n",
    "        n\n",
    "        for n, p in net.named_parameters()\n",
    "        if not n.endswith(\".quantiles\") and p.requires_grad\n",
    "    }\n",
    "    aux_parameters = {\n",
    "        n\n",
    "        for n, p in net.named_parameters()\n",
    "        if n.endswith(\".quantiles\") and p.requires_grad\n",
    "    }\n",
    "\n",
    "    # Make sure we don't have an intersection of parameters\n",
    "    params_dict = dict(net.named_parameters())\n",
    "    inter_params = parameters & aux_parameters\n",
    "    union_params = parameters | aux_parameters\n",
    "\n",
    "    assert len(inter_params) == 0\n",
    "    assert len(union_params) - len(params_dict.keys()) == 0\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(parameters)),\n",
    "        lr=learning_rate,\n",
    "    )\n",
    "    aux_optimizer = optim.Adam(\n",
    "        (params_dict[n] for n in sorted(aux_parameters)),\n",
    "        lr=aux_learning_rate,\n",
    "    )\n",
    "    return optimizer, aux_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae22104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model, criterion, train_dataloader, optimizer, aux_optimizer, epoch, clip_max_norm, metric\n",
    "):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for i, d in enumerate(train_dataloader):\n",
    "        d = d.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        aux_optimizer.zero_grad()\n",
    "\n",
    "        out_net = model(d)\n",
    "\n",
    "        out_criterion = criterion(out_net, d)\n",
    "        out_criterion[\"loss\"].backward()\n",
    "        if clip_max_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        aux_loss = model.aux_loss()\n",
    "        aux_loss.backward()\n",
    "        aux_optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\n",
    "                f\"Train epoch {epoch}: [\"\n",
    "                f\"{i*len(d)}/{len(train_dataloader.dataset)}\"\n",
    "                f\" ({100. * i / len(train_dataloader):.0f}%)]\"\n",
    "                f'\\tLoss: {out_criterion[\"loss\"].item():.3f} |'\n",
    "                f'\\t{metric.upper()} loss: {out_criterion[metric + \"_loss\"].item():.3f} |'\n",
    "                f'\\tBpp loss: {out_criterion[\"bpp_loss\"].item():.2f} |'\n",
    "                f\"\\tAux loss: {aux_loss.item():.2f}\"\n",
    "            )\n",
    "            wandb.log({\"loss\": out_criterion[\"loss\"].item()})\n",
    "            wandb.log({\"loss_\" + metric: out_criterion[metric + \"_loss\"].item()})\n",
    "            wandb.log({\"loss_bpp\": out_criterion[\"bpp_loss\"].item()})\n",
    "            wandb.log({\"loss_aux\": aux_loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d858d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(epoch, test_dataloader, model, criterion, metric):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    loss = AverageMeter()\n",
    "    bpp_loss = AverageMeter()\n",
    "    metric_loss = AverageMeter()\n",
    "    aux_loss = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in test_dataloader:\n",
    "            d = d.to(device)\n",
    "            out_net = model(d)\n",
    "            out_criterion = criterion(out_net, d)\n",
    "\n",
    "            aux_loss.update(model.aux_loss())\n",
    "            bpp_loss.update(out_criterion[\"bpp_loss\"])\n",
    "            loss.update(out_criterion[\"loss\"])\n",
    "            metric_loss.update(out_criterion[metric + \"_loss\"])\n",
    "\n",
    "    print(\n",
    "        f\"Test epoch {epoch}: Average losses:\"\n",
    "        f\"\\tLoss: {loss.avg:.3f} |\"\n",
    "        f\"\\t{metric.upper()} loss: {metric_loss.avg:.3f} |\"\n",
    "        f\"\\tBpp loss: {bpp_loss.avg:.2f} |\"\n",
    "        f\"\\tAux loss: {aux_loss.avg:.2f}\\n\"\n",
    "    )\n",
    "\n",
    "    return loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b024e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename):\n",
    "    torch.save(state, filename + \".pth.tar\")\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + \".pth.tar\", filename + \"_best_loss.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc337d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (512, 512)\n",
    "dataset = \"/home/clemens/Documents/TU Wien/2021W/Bachelor Thesis/datasets/selection\"\n",
    "model_dir = \"/home/clemens/Documents/TU Wien/2021W/Bachelor Thesis/final_training\"\n",
    "batch_size = 1\n",
    "test_batch_size=2\n",
    "learning_rate=1e-4\n",
    "aux_learning_rate=1e-4\n",
    "num_workers = 8\n",
    "save = True\n",
    "clip_max_norm=1.0\n",
    "wandb_project = \"Synthetic Image Compression\"\n",
    "\n",
    "epoch_split=1\n",
    "epoch_final=2\n",
    "model = \"mbt2018\"\n",
    "lmbda = {\n",
    "    'mse': {\n",
    "        1: 0.0018,\n",
    "        2: 0.0035,\n",
    "        3: 0.0067,\n",
    "        4: 0.0130,\n",
    "        5: 0.0250,\n",
    "        6: 0.0483,\n",
    "        7: 0.0932,\n",
    "        8: 0.1800\n",
    "    },\n",
    "    'msssim': {\n",
    "        1: 2.4,\n",
    "        2: 4.58,\n",
    "        3: 8.73,\n",
    "        4: 16.64,\n",
    "        5: 31.37,\n",
    "        6: 60.5,\n",
    "        7: 115.37,\n",
    "        8: 220\n",
    "    }\n",
    "}\n",
    "\n",
    "wandb_ids = {\n",
    "    'base': {},\n",
    "    'fine_mse': {},\n",
    "    'fine_msssim': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3cddbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epoch_split,\n",
    "  \"batch_size\": batch_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1ca893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(model, quality, loss_fn, target_epochs):\n",
    "    return model + \"_q\" + str(quality) + \"_\" + loss_fn + \"_\" + str(target_epochs) + \"ep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "435d5cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint_if_exists(filename, net, optimizer, aux_optimizer, lr_scheduler, device):\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading\", filename)\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        last_epoch = checkpoint[\"epoch\"] + 1\n",
    "        net.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        aux_optimizer.load_state_dict(checkpoint[\"aux_optimizer\"])\n",
    "        lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "        return last_epoch\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019e9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base(model, quality, lmbda, train_dataloader, test_dataloader):\n",
    "    print(\"Training \" + model + \" at quality \" + str(quality) + \" from scratch; lambda=\" + str(lmbda))\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    net = models[model](quality=quality)\n",
    "    net = net.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = CustomDataParallel(net)\n",
    "    \n",
    "    optimizer, aux_optimizer = configure_optimizers(net, learning_rate, aux_learning_rate)\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "    criterion = RateDistortionLossMSE(lmbda=lmbda)\n",
    "    \n",
    "    last_epoch = 0\n",
    "    \n",
    "    model_file_base = model_dir + '/' + create_filename(model, quality, \"mse\", epoch_split)\n",
    "        \n",
    "    last_epoch = load_checkpoint_if_exists(\n",
    "        model_file_base + \".pth.tar\", net, optimizer, aux_optimizer, lr_scheduler, device\n",
    "    )\n",
    "    if last_epoch >= epoch_split:\n",
    "        print(\"Found checkpoint for this model with \" + str(last_epoch) + \" epochs - nothing to do\")\n",
    "        return\n",
    "    \n",
    "    if quality in wandb_ids['fine_' + metric]:\n",
    "        wandb.init(project=wandb_project, entity=\"cmw98\", resume=\"allow\", reinit=True, id=wandb_ids['fine_' + metric][quality])\n",
    "    else:\n",
    "        wandb.init(project=wandb_project, entity=\"cmw98\", resume=\"allow\", reinit=True)\n",
    "        wandb.run.name = model + \"_q\" + str(quality) + \"_\" + metric + \"_adapted_lmbda\"    \n",
    "    wandb.watch(net, criterion=criterion, log=\"gradients\", log_freq=1, log_graph=(False))\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in range(last_epoch, epoch_split):\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        train_one_epoch(\n",
    "            net,\n",
    "            criterion,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            aux_optimizer,\n",
    "            epoch,\n",
    "            clip_max_norm,\n",
    "            \"mse\"\n",
    "        )\n",
    "        loss = test_epoch(epoch, test_dataloader, net, criterion, \"mse\")\n",
    "        lr_scheduler.step(loss)\n",
    "        \n",
    "        is_best = loss < best_loss\n",
    "        best_loss = min(loss, best_loss)\n",
    "        \n",
    "        if save:\n",
    "            save_checkpoint(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"state_dict\": net.state_dict(),\n",
    "                    \"loss\": loss,\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"aux_optimizer\": aux_optimizer.state_dict(),\n",
    "                    \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                },\n",
    "                is_best,\n",
    "                filename=model_file_base\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03386aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine(model, quality, lmbda, train_dataloader, test_dataloader, metric):\n",
    "    print(\"Finetuning \" + model + \" at quality \" + str(quality) + \" for \" + metric + \"; lambda=\" + str(lmbda))\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    net = models[model](quality=quality)\n",
    "    net = net.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = CustomDataParallel(net)\n",
    "    \n",
    "    optimizer, aux_optimizer = configure_optimizers(net, learning_rate, aux_learning_rate)\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\n",
    "    criterion = (RateDistortionLossMSE(lmbda=lmbda) if metric == 'mse' else RateDistortionLossMSSSIM(lmbda=lmbda))\n",
    "\n",
    "    last_epoch = 0\n",
    "\n",
    "    model_file_base = model_dir + '/' + create_filename(model, quality, metric, epoch_final)\n",
    "    \n",
    "    last_epoch = load_checkpoint_if_exists(\n",
    "        model_file_base + \".pth.tar\", net, optimizer, aux_optimizer, lr_scheduler, device\n",
    "    )\n",
    "    if last_epoch < epoch_split:\n",
    "        last_epoch = load_checkpoint_if_exists(\n",
    "            model_dir + '/' + create_filename(model, quality, 'mse', epoch_split) + \".pth.tar\",\n",
    "            net, optimizer, aux_optimizer, lr_scheduler, device\n",
    "        )\n",
    "    if last_epoch < epoch_split or last_epoch >= epoch_final:\n",
    "        print(\"Base model is at \" + str(last_epoch) + \" epochs - aborting\")\n",
    "        return\n",
    "    \n",
    "    if quality in wandb_ids['fine_' + metric]:\n",
    "        wandb.init(project=wandb_project, entity=\"cmw98\", resume=\"allow\", reinit=True, id=wandb_ids['fine_' + metric][quality])\n",
    "    else:\n",
    "        wandb.init(project=wandb_project, entity=\"cmw98\", resume=\"allow\", reinit=True)\n",
    "        wandb.run.name = model + \"_q\" + str(quality) + \"_\" + metric + \"_adapted_lmbda\"    \n",
    "    wandb.watch(net, criterion=criterion, log=\"gradients\", log_freq=1, log_graph=(False))\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in range(last_epoch, epoch_final):\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        train_one_epoch(\n",
    "            net,\n",
    "            criterion,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            aux_optimizer,\n",
    "            epoch,\n",
    "            clip_max_norm,\n",
    "            metric\n",
    "        )\n",
    "        loss = test_epoch(epoch, test_dataloader, net, criterion, metric)\n",
    "        lr_scheduler.step(loss)\n",
    "        \n",
    "        is_best = loss < best_loss\n",
    "        best_loss = min(loss, best_loss)\n",
    "        \n",
    "        if save:\n",
    "            save_checkpoint(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"state_dict\": net.state_dict(),\n",
    "                    \"loss\": loss,\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"aux_optimizer\": aux_optimizer.state_dict(),\n",
    "                    \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "                },\n",
    "                is_best,\n",
    "                filename=model_file_base\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "843884de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mbt2018 at quality 4 from scratch; lambda=0.013\n",
      "Loading /home/clemens/Documents/TU Wien/2021W/Bachelor Thesis/final_training/mbt2018_q4_mse_1ep.pth.tar\n",
      "Found checkpoint for this model with 1 epochs - nothing to do\n",
      "Finetuning mbt2018 at quality 4 for mse; lambda=0.013\n",
      "Loading /home/clemens/Documents/TU Wien/2021W/Bachelor Thesis/final_training/mbt2018_q4_mse_2ep.pth.tar\n",
      "Base model is at 2 epochs - aborting\n",
      "Finetuning mbt2018 at quality 4 for msssim; lambda=16.64\n",
      "Loading /home/clemens/Documents/TU Wien/2021W/Bachelor Thesis/final_training/mbt2018_q4_mse_1ep.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcmw98\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/cmw98/Synthetic%20Image%20Compression/runs/1lcnk2ip\" target=\"_blank\">graceful-hill-71</a></strong> to <a href=\"https://wandb.ai/cmw98/Synthetic%20Image%20Compression\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Train epoch 1: [0/1056 (0%)]\tLoss: 2.736 |\tMSSSIM loss: 0.133 |\tBpp loss: 0.52 |\tAux loss: 7560.81\n",
      "Train epoch 1: [10/1056 (1%)]\tLoss: 4.140 |\tMSSSIM loss: 0.208 |\tBpp loss: 0.69 |\tAux loss: 7549.15\n",
      "Train epoch 1: [20/1056 (2%)]\tLoss: 3.205 |\tMSSSIM loss: 0.147 |\tBpp loss: 0.75 |\tAux loss: 7536.09\n",
      "Train epoch 1: [30/1056 (3%)]\tLoss: 2.070 |\tMSSSIM loss: 0.089 |\tBpp loss: 0.60 |\tAux loss: 7520.59\n",
      "Train epoch 1: [40/1056 (4%)]\tLoss: 3.431 |\tMSSSIM loss: 0.143 |\tBpp loss: 1.05 |\tAux loss: 7507.27\n",
      "Train epoch 1: [50/1056 (5%)]\tLoss: 2.453 |\tMSSSIM loss: 0.107 |\tBpp loss: 0.67 |\tAux loss: 7495.58\n",
      "Train epoch 1: [60/1056 (6%)]\tLoss: 4.286 |\tMSSSIM loss: 0.193 |\tBpp loss: 1.07 |\tAux loss: 7484.62\n",
      "Train epoch 1: [70/1056 (7%)]\tLoss: 2.006 |\tMSSSIM loss: 0.090 |\tBpp loss: 0.51 |\tAux loss: 7475.61\n",
      "Train epoch 1: [80/1056 (8%)]\tLoss: 1.412 |\tMSSSIM loss: 0.057 |\tBpp loss: 0.46 |\tAux loss: 7464.71\n",
      "Train epoch 1: [90/1056 (9%)]\tLoss: 2.964 |\tMSSSIM loss: 0.133 |\tBpp loss: 0.76 |\tAux loss: 7452.88\n",
      "Train epoch 1: [100/1056 (9%)]\tLoss: 2.639 |\tMSSSIM loss: 0.128 |\tBpp loss: 0.50 |\tAux loss: 7441.47\n",
      "Train epoch 1: [110/1056 (10%)]\tLoss: 1.946 |\tMSSSIM loss: 0.079 |\tBpp loss: 0.62 |\tAux loss: 7431.47\n",
      "Train epoch 1: [120/1056 (11%)]\tLoss: 4.557 |\tMSSSIM loss: 0.225 |\tBpp loss: 0.81 |\tAux loss: 7421.28\n",
      "Train epoch 1: [130/1056 (12%)]\tLoss: 1.511 |\tMSSSIM loss: 0.056 |\tBpp loss: 0.58 |\tAux loss: 7411.47\n",
      "Train epoch 1: [140/1056 (13%)]\tLoss: 1.811 |\tMSSSIM loss: 0.076 |\tBpp loss: 0.55 |\tAux loss: 7402.31\n",
      "Train epoch 1: [150/1056 (14%)]\tLoss: 2.056 |\tMSSSIM loss: 0.087 |\tBpp loss: 0.60 |\tAux loss: 7391.70\n",
      "Train epoch 1: [160/1056 (15%)]\tLoss: 1.313 |\tMSSSIM loss: 0.045 |\tBpp loss: 0.57 |\tAux loss: 7379.84\n",
      "Train epoch 1: [170/1056 (16%)]\tLoss: 1.507 |\tMSSSIM loss: 0.059 |\tBpp loss: 0.52 |\tAux loss: 7368.06\n",
      "Train epoch 1: [180/1056 (17%)]\tLoss: 3.162 |\tMSSSIM loss: 0.147 |\tBpp loss: 0.71 |\tAux loss: 7357.01\n",
      "Train epoch 1: [190/1056 (18%)]\tLoss: 1.970 |\tMSSSIM loss: 0.084 |\tBpp loss: 0.56 |\tAux loss: 7347.16\n",
      "Train epoch 1: [200/1056 (19%)]\tLoss: 1.716 |\tMSSSIM loss: 0.072 |\tBpp loss: 0.51 |\tAux loss: 7337.53\n",
      "Train epoch 1: [210/1056 (20%)]\tLoss: 1.481 |\tMSSSIM loss: 0.055 |\tBpp loss: 0.56 |\tAux loss: 7328.66\n",
      "Train epoch 1: [220/1056 (21%)]\tLoss: 3.034 |\tMSSSIM loss: 0.138 |\tBpp loss: 0.74 |\tAux loss: 7318.78\n",
      "Train epoch 1: [230/1056 (22%)]\tLoss: 1.986 |\tMSSSIM loss: 0.089 |\tBpp loss: 0.50 |\tAux loss: 7308.46\n",
      "Train epoch 1: [240/1056 (23%)]\tLoss: 1.777 |\tMSSSIM loss: 0.069 |\tBpp loss: 0.63 |\tAux loss: 7299.15\n",
      "Train epoch 1: [250/1056 (24%)]\tLoss: 1.606 |\tMSSSIM loss: 0.067 |\tBpp loss: 0.49 |\tAux loss: 7289.69\n",
      "Train epoch 1: [260/1056 (25%)]\tLoss: 1.642 |\tMSSSIM loss: 0.072 |\tBpp loss: 0.44 |\tAux loss: 7280.62\n",
      "Train epoch 1: [270/1056 (26%)]\tLoss: 1.859 |\tMSSSIM loss: 0.080 |\tBpp loss: 0.53 |\tAux loss: 7271.95\n",
      "Train epoch 1: [280/1056 (27%)]\tLoss: 3.454 |\tMSSSIM loss: 0.155 |\tBpp loss: 0.87 |\tAux loss: 7261.90\n",
      "Train epoch 1: [290/1056 (27%)]\tLoss: 1.592 |\tMSSSIM loss: 0.065 |\tBpp loss: 0.51 |\tAux loss: 7252.85\n",
      "Train epoch 1: [300/1056 (28%)]\tLoss: 1.464 |\tMSSSIM loss: 0.057 |\tBpp loss: 0.52 |\tAux loss: 7244.29\n",
      "Train epoch 1: [310/1056 (29%)]\tLoss: 1.786 |\tMSSSIM loss: 0.066 |\tBpp loss: 0.68 |\tAux loss: 7236.53\n",
      "Train epoch 1: [320/1056 (30%)]\tLoss: 2.019 |\tMSSSIM loss: 0.082 |\tBpp loss: 0.65 |\tAux loss: 7229.31\n",
      "Train epoch 1: [330/1056 (31%)]\tLoss: 1.350 |\tMSSSIM loss: 0.053 |\tBpp loss: 0.47 |\tAux loss: 7220.63\n",
      "Train epoch 1: [340/1056 (32%)]\tLoss: 3.957 |\tMSSSIM loss: 0.183 |\tBpp loss: 0.91 |\tAux loss: 7211.09\n",
      "Train epoch 1: [350/1056 (33%)]\tLoss: 2.798 |\tMSSSIM loss: 0.118 |\tBpp loss: 0.84 |\tAux loss: 7202.92\n",
      "Train epoch 1: [360/1056 (34%)]\tLoss: 1.325 |\tMSSSIM loss: 0.052 |\tBpp loss: 0.46 |\tAux loss: 7194.09\n",
      "Train epoch 1: [370/1056 (35%)]\tLoss: 1.717 |\tMSSSIM loss: 0.070 |\tBpp loss: 0.54 |\tAux loss: 7184.28\n",
      "Train epoch 1: [380/1056 (36%)]\tLoss: 2.071 |\tMSSSIM loss: 0.086 |\tBpp loss: 0.63 |\tAux loss: 7175.83\n",
      "Train epoch 1: [390/1056 (37%)]\tLoss: 1.103 |\tMSSSIM loss: 0.039 |\tBpp loss: 0.45 |\tAux loss: 7165.91\n",
      "Train epoch 1: [400/1056 (38%)]\tLoss: 1.353 |\tMSSSIM loss: 0.050 |\tBpp loss: 0.51 |\tAux loss: 7155.14\n",
      "Train epoch 1: [410/1056 (39%)]\tLoss: 2.104 |\tMSSSIM loss: 0.089 |\tBpp loss: 0.62 |\tAux loss: 7143.72\n",
      "Train epoch 1: [420/1056 (40%)]\tLoss: 2.298 |\tMSSSIM loss: 0.100 |\tBpp loss: 0.63 |\tAux loss: 7134.79\n",
      "Train epoch 1: [430/1056 (41%)]\tLoss: 1.592 |\tMSSSIM loss: 0.066 |\tBpp loss: 0.49 |\tAux loss: 7124.77\n",
      "Train epoch 1: [440/1056 (42%)]\tLoss: 1.250 |\tMSSSIM loss: 0.046 |\tBpp loss: 0.49 |\tAux loss: 7114.61\n",
      "Train epoch 1: [450/1056 (43%)]\tLoss: 1.733 |\tMSSSIM loss: 0.060 |\tBpp loss: 0.74 |\tAux loss: 7105.43\n",
      "Train epoch 1: [460/1056 (44%)]\tLoss: 1.676 |\tMSSSIM loss: 0.059 |\tBpp loss: 0.70 |\tAux loss: 7096.13\n",
      "Train epoch 1: [470/1056 (45%)]\tLoss: 1.325 |\tMSSSIM loss: 0.047 |\tBpp loss: 0.54 |\tAux loss: 7087.11\n",
      "Train epoch 1: [480/1056 (45%)]\tLoss: 1.249 |\tMSSSIM loss: 0.046 |\tBpp loss: 0.48 |\tAux loss: 7079.55\n",
      "Train epoch 1: [490/1056 (46%)]\tLoss: 1.303 |\tMSSSIM loss: 0.048 |\tBpp loss: 0.51 |\tAux loss: 7069.64\n",
      "Train epoch 1: [500/1056 (47%)]\tLoss: 1.300 |\tMSSSIM loss: 0.047 |\tBpp loss: 0.51 |\tAux loss: 7057.97\n",
      "Train epoch 1: [510/1056 (48%)]\tLoss: 2.420 |\tMSSSIM loss: 0.090 |\tBpp loss: 0.93 |\tAux loss: 7046.91\n",
      "Train epoch 1: [520/1056 (49%)]\tLoss: 1.570 |\tMSSSIM loss: 0.055 |\tBpp loss: 0.66 |\tAux loss: 7036.44\n",
      "Train epoch 1: [530/1056 (50%)]\tLoss: 2.384 |\tMSSSIM loss: 0.094 |\tBpp loss: 0.81 |\tAux loss: 7026.73\n",
      "Train epoch 1: [540/1056 (51%)]\tLoss: 1.942 |\tMSSSIM loss: 0.081 |\tBpp loss: 0.59 |\tAux loss: 7018.08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26783/3588609931.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtrain_fine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_lmbda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtrain_fine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_lmbda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'msssim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_26783/1101615344.py\u001b[0m in \u001b[0;36mtrain_fine\u001b[0;34m(model, quality, lmbda, train_dataloader, test_dataloader, metric)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Learning rate: {optimizer.param_groups[0]['lr']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         train_one_epoch(\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26783/785695893.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, train_dataloader, optimizer, aux_optimizer, epoch, clip_max_norm, metric)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mout_criterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mout_criterion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclip_max_norm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_max_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [transforms.RandomCrop(patch_size), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [transforms.CenterCrop(patch_size), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder(dataset, split=\"train\", transform=train_transforms)\n",
    "test_dataset = ImageFolder(dataset, split=\"test\", transform=test_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    "    pin_memory=(torch.cuda.is_available()),\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=(torch.cuda.is_available()),\n",
    ")\n",
    "\n",
    "for quality in [4, 6, 8]:\n",
    "    for metric in ['mse', 'msssim']:\n",
    "        current_lmbda = lmbda[metric][quality]\n",
    "        if metric == 'mse':\n",
    "            train_base(model, quality, current_lmbda, train_dataloader, test_dataloader)\n",
    "            train_fine(model, quality, current_lmbda, train_dataloader, test_dataloader, 'mse')\n",
    "        else:\n",
    "            train_fine(model, quality, current_lmbda, train_dataloader, test_dataloader, 'msssim')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
